{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "g5AJkD1pZt2F"
   },
   "source": [
    "# Lab 6: PyTorch\n",
    "**Week 1, Day 1: Basics and PyTorch**\n",
    "\n",
    "**By Neuromatch Academy** and adapted by Prof. Nils Murrugarra from Univ of Pittsburgh.\n",
    "\n",
    "\n",
    "__Content creators:__ Shubh Pachchigar, Vladimir Haltakov, Matthew Sargent, Konrad Kording\n",
    "\n",
    "__Content reviewers:__ Deepak Raya, Siwei Bai, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Anoop Kulkarni, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Arush Tagade, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "l9M4GjveZt2G"
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "q-iA9KHpZt2G"
   },
   "source": [
    "Throughout your Neuromatch tutorials, most (probably all!) notebooks contain setup cells. These cells will import the required Python packages (e.g., PyTorch, NumPy); set global or environment variables, and load in helper functions for things like plotting. In some tutorials, you will notice that we install some dependencies even if they are preinstalled on Google Colab or Kaggle. This happens because we have added automation to our repository through [GitHub Actions](https://docs.github.com/en/actions/learn-github-actions/introduction-to-github-actions).\n",
    "\n",
    "Be sure to run all of the cells in the setup section. Feel free to expand them and have a look at what you are loading in, but you should be able to fulfill the learning objectives of every tutorial without having to look at these cells.\n",
    "\n",
    "If you start building your own projects built on this code base we highly recommend looking at them in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {},
    "id": "buB3wmIfZt2G"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "eHPOh_n4Zt2G",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "16203391-46aa-441f-ea29-cb058bb85c6d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m32.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for vibecheck (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Building wheel for datatops (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# @title Install and import feedback gadget\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_dl\",\n",
    "            \"user_key\": \"f379rz8y\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W1D1_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {},
    "id": "WKdYy_N4Zt2G"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {},
    "id": "akpBA45zZt2G",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import ipywidgets as widgets\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {},
    "id": "UlxQzslXZt2G",
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "# @title Helper Functions\n",
    "\n",
    "def checkExercise1(A, B, C, D):\n",
    "  \"\"\"\n",
    "  Helper function for checking Exercise 1.\n",
    "\n",
    "  Args:\n",
    "    A: torch.Tensor\n",
    "      Torch Tensor of shape (20, 21) consisting of ones.\n",
    "    B: torch.Tensor\n",
    "      Torch Tensor of size([3,4])\n",
    "    C: torch.Tensor\n",
    "      Torch Tensor of size([20,21])\n",
    "    D: torch.Tensor\n",
    "      Torch Tensor of size([19])\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  assert torch.equal(A.to(int),torch.ones(20, 21).to(int)), \"Got: {A} \\n Expected: {torch.ones(20, 21)} (shape: {torch.ones(20, 21).shape})\"\n",
    "  assert np.array_equal(B.numpy(),np.vander([1, 2, 3], 4)), \"Got: {B} \\n Expected: {np.vander([1, 2, 3], 4)} (shape: {np.vander([1, 2, 3], 4).shape})\"\n",
    "  assert C.shape == (20, 21), \"Got: {C} \\n Expected (shape: {(20, 21)})\"\n",
    "  assert torch.equal(D, torch.arange(4, 41, step=2)), \"Got {D} \\n Expected: {torch.arange(4, 41, step=2)} (shape: {torch.arange(4, 41, step=2).shape})\"\n",
    "  print(\"All correct\")\n",
    "\n",
    "def timeFun(f, dim, iterations, device='cpu'):\n",
    "  \"\"\"\n",
    "  Helper function to calculate amount of time taken per instance on CPU/GPU\n",
    "\n",
    "  Args:\n",
    "    f: BufferedReader IO instance\n",
    "      Function name for which to calculate computational time complexity\n",
    "    dim: Integer\n",
    "      Number of dimensions in instance in question\n",
    "    iterations: Integer\n",
    "      Number of iterations for instance in question\n",
    "    device: String\n",
    "      Device on which respective computation is to be run\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  iterations = iterations\n",
    "  t_total = 0\n",
    "  for _ in range(iterations):\n",
    "    start = time.time()\n",
    "    f(dim, device)\n",
    "    end = time.time()\n",
    "    t_total += end - start\n",
    "\n",
    "  if device == 'cpu':\n",
    "    print(f\"time taken for {iterations} iterations of {f.__name__}({dim}, {device}): {t_total:.5f}\")\n",
    "  else:\n",
    "    print(f\"time taken for {iterations} iterations of {f.__name__}({dim}, {device}): {t_total:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "PwsEPmNLZt2H"
   },
   "source": [
    "**Important note: Colab users**\n",
    "\n",
    "*Scratch Code Cells*\n",
    "\n",
    "If you want to quickly try out something or take a look at the data, you can use scratch code cells. They allow you to run Python code, but will not mess up the structure of your notebook.\n",
    "\n",
    "To open a new scratch cell go to *Insert* → *Scratch code cell*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "V5k5KcZcZt2H"
   },
   "source": [
    "---\n",
    "# Section 1: The Basics of PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "KZD9_H-lZt2H"
   },
   "source": [
    "PyTorch is a Python-based scientific computing package targeted at two sets of\n",
    "audiences:\n",
    "\n",
    "-  A replacement for NumPy optimized for the power of GPUs\n",
    "-  A deep learning platform that provides significant flexibility\n",
    "   and speed\n",
    "\n",
    "At its core, PyTorch provides a few key features:\n",
    "\n",
    "- A multidimensional [Tensor](https://pytorch.org/docs/stable/tensors.html) object, similar to [NumPy Array](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) but with GPU acceleration.\n",
    "- An optimized **autograd** engine for automatically computing derivatives.\n",
    "- A clean, modular API for building and deploying **deep learning models**.\n",
    "\n",
    "You can find more information about PyTorch in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "H_UQ9tlUZt2H"
   },
   "source": [
    "## Section 1.1: Creating Tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "3nlc2vvYZt2H"
   },
   "source": [
    "There are various ways of creating tensors, and when doing any real deep learning project, we will usually have to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Dz7EtI__Zt2H"
   },
   "source": [
    "**Construct tensors directly:**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {},
    "id": "5SjX-oU1Zt2H",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f8c3ea7a-8798-4ef7-8594-9c17bafe6aa0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor a: tensor([0, 1, 2])\n",
      "Tensor b: tensor([[1.0000, 1.1000],\n",
      "        [1.2000, 1.3000]])\n",
      "Tensor c: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# We can construct a tensor directly from some common python iterables,\n",
    "# such as list and tuple nested iterables can also be handled as long as the\n",
    "# dimensions are compatible\n",
    "\n",
    "# tensor from a list\n",
    "a = torch.tensor([0, 1, 2])\n",
    "\n",
    "#tensor from a tuple of tuples\n",
    "b = ((1.0, 1.1), (1.2, 1.3))\n",
    "b = torch.tensor(b)\n",
    "\n",
    "# tensor from a numpy array\n",
    "c = np.ones([2, 3])\n",
    "c = torch.tensor(c)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "print(f\"Tensor c: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "_xSCwxFvZt2I"
   },
   "source": [
    "**Some common tensor constructors:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {},
    "id": "8nqf0nd5Zt2I",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9c169ae9-c31a-44e6-d2de-6708555ff3fd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor x: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Tensor y: tensor([0., 0.])\n",
      "Tensor z: tensor([[[1.2040e-27, 0.0000e+00, 1.5119e-23, 0.0000e+00, 1.1210e-43]]])\n"
     ]
    }
   ],
   "source": [
    "# The numerical arguments we pass to these constructors\n",
    "# determine the shape of the output tensor\n",
    "\n",
    "x = torch.ones(5, 3)\n",
    "y = torch.zeros(2)\n",
    "z = torch.empty(1, 1, 5)\n",
    "print(f\"Tensor x: {x}\")\n",
    "print(f\"Tensor y: {y}\")\n",
    "print(f\"Tensor z: {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "qxETB2T1Zt2I"
   },
   "source": [
    "Notice that `.empty()` does not return zeros, but seemingly random numbers. Unlike `.zeros()`, which initialises the elements of the tensor with zeros, `.empty()` just allocates the memory. It is hence a bit faster if you are looking to just create a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "qFZThA1MZt2I"
   },
   "source": [
    "**Creating random tensors and tensors like other tensors:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {},
    "id": "7_fklTtbZt2I",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4a2e6d24-b831-45bc-abac-417d6e2e7b2b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor a: tensor([[0.0179, 0.9347, 0.1699]])\n",
      "Tensor b: tensor([[-0.3410,  0.9777, -1.8761, -1.0040],\n",
      "        [ 1.8509,  0.2466, -0.0618, -0.8970],\n",
      "        [ 0.6473,  0.6336,  2.0903,  0.2089]])\n",
      "Tensor c: tensor([[0., 0., 0.]])\n",
      "Tensor d: tensor([[0.9622, 0.7327, 0.1276]])\n"
     ]
    }
   ],
   "source": [
    "# There are also constructors for random numbers\n",
    "\n",
    "# Uniform distribution\n",
    "a = torch.rand(1, 3)\n",
    "\n",
    "# Normal distribution\n",
    "b = torch.randn(3, 4)\n",
    "\n",
    "# There are also constructors that allow us to construct\n",
    "# a tensor according to the above constructors, but with\n",
    "# dimensions equal to another tensor.\n",
    "\n",
    "c = torch.zeros_like(a)\n",
    "d = torch.rand_like(c)\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "print(f\"Tensor c: {c}\")\n",
    "print(f\"Tensor d: {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "lkeH1PuSZt2I"
   },
   "source": [
    "*Reproducibility*:\n",
    "\n",
    "- PyTorch Random Number Generator (RNG): You can use `torch.manual_seed()` to seed the RNG for all devices (both CPU and GPU):\n",
    "\n",
    "```python\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "```\n",
    "- For custom operators, you might need to set python seed as well:\n",
    "\n",
    "```python\n",
    "import random\n",
    "random.seed(0)\n",
    "```\n",
    "\n",
    "- Random number generators in other libraries (e.g., NumPy):\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "nkLalPVqZt2I"
   },
   "source": [
    "Here, we define for you a function called `set_seed` that does the job for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {},
    "id": "Lkqjj2BuZt2I"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "T7oY8KDxZt2I"
   },
   "source": [
    "Now, let's use the `set_seed` function in the previous example. Execute the cell multiple times to verify that the numbers printed are always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {},
    "id": "GdNYKspYZt2I"
   },
   "outputs": [],
   "source": [
    "def simplefun(seed=True, my_seed=None):\n",
    "  \"\"\"\n",
    "  Helper function to verify effectiveness of set_seed attribute\n",
    "\n",
    "  Args:\n",
    "    seed: Boolean\n",
    "      Specifies if seed value is provided or not\n",
    "    my_seed: Integer\n",
    "      Initializes seed to specified value\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if seed:\n",
    "    set_seed(seed=my_seed)\n",
    "\n",
    "  # uniform distribution\n",
    "  a = torch.rand(1, 3)\n",
    "  # normal distribution\n",
    "  b = torch.randn(3, 4)\n",
    "\n",
    "  print(\"Tensor a: \", a)\n",
    "  print(\"Tensor b: \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {},
    "id": "UYAiycyXZt2I",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "95aaca1e-e152-43fd-bc0e-157af5aeaba1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random seed 0 has been set.\n",
      "Tensor a:  tensor([[0.4963, 0.7682, 0.0885]])\n",
      "Tensor b:  tensor([[ 0.3643,  0.1344,  0.1642,  0.3058],\n",
      "        [ 0.2100,  0.9056,  0.6035,  0.8110],\n",
      "        [-0.0451,  0.8797,  1.0482, -0.0445]])\n"
     ]
    }
   ],
   "source": [
    "simplefun(seed=True, my_seed=0)  # Turn `seed` to `False` or change `my_seed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Q_1Y0j22Zt2I"
   },
   "source": [
    "**Numpy-like number ranges:**\n",
    "---\n",
    "The ```.arange()``` and ```.linspace()``` behave how you would expect them to if you are familar with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {},
    "id": "sDVh4ZefZt2I",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b61f20dd-5cb4-4151-a4a9-48a8a1d46b9e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tensor a: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "\n",
      "Numpy array b: [0 1 2 3 4 5 6 7 8 9]\n",
      "\n",
      "Tensor c: tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n",
      "        4.5000, 5.0000])\n",
      "\n",
      "Numpy array d: [0.  0.5 1.  1.5 2.  2.5 3.  3.5 4.  4.5 5. ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 10, step=1)\n",
    "b = np.arange(0, 10, step=1)\n",
    "\n",
    "c = torch.linspace(0, 5, steps=11)\n",
    "d = np.linspace(0, 5, num=11)\n",
    "\n",
    "print(f\"Tensor a: {a}\\n\")\n",
    "print(f\"Numpy array b: {b}\\n\")\n",
    "print(f\"Tensor c: {c}\\n\")\n",
    "print(f\"Numpy array d: {d}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "2l63dYCXZt2L"
   },
   "source": [
    "### Coding Exercise 1.1: Creating Tensors\n",
    "\n",
    "Below you will find some incomplete code. Fill in the missing code to construct the specified tensors.\n",
    "\n",
    "We want the tensors:\n",
    "\n",
    "$A:$ 20 by 21 tensor consisting of ones\n",
    "\n",
    "$B:$ a tensor with elements equal to the elements of numpy array $Z$\n",
    "\n",
    "$C:$ a tensor with the same number of elements as $A$ but with values $\n",
    "\\sim \\mathcal{U}(0,1)^\\dagger$\n",
    "\n",
    "$D:$ a 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "\n",
    "<br>\n",
    "\n",
    "$^\\dagger$: $\\mathcal{U(\\alpha, \\beta)}$ denotes the [uniform distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) from $\\alpha$ to $\\beta$, with $\\alpha, \\beta \\in \\mathbb{R}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "wOafAxdpZt2L"
   },
   "source": [
    "```python\n",
    "def tensor_creation(Z):\n",
    "  \"\"\"\n",
    "  A function that creates various tensors.\n",
    "\n",
    "  Args:\n",
    "    Z: numpy.ndarray\n",
    "      An array of shape (3,4)\n",
    "\n",
    "  Returns:\n",
    "    A : Tensor\n",
    "      20 by 21 tensor consisting of ones\n",
    "    B : Tensor\n",
    "      A tensor with elements equal to the elements of numpy array Z\n",
    "    C : Tensor\n",
    "      A tensor with the same number of elements as A but with values ∼U(0,1)\n",
    "    D : Tensor\n",
    "      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "  \"\"\"\n",
    "  #################################################\n",
    "  ## TODO for students: fill in the missing code\n",
    "  ## from the first expression\n",
    "  raise NotImplementedError(\"Student exercise: say what they should have done\")\n",
    "  #################################################\n",
    "  A = ...\n",
    "  B = ...\n",
    "  C = ...\n",
    "  D = ...\n",
    "\n",
    "  return A, B, C, D\n",
    "\n",
    "\n",
    "# numpy array to copy later\n",
    "Z = np.vander([1, 2, 3], 4)\n",
    "\n",
    "# Uncomment below to check your function!\n",
    "# A, B, C, D = tensor_creation(Z)\n",
    "# checkExercise1(A, B, C, D)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {},
    "id": "b8GeJ35hZt2L",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6ef0f344-13ec-42ef-fb80-d024b74b564d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "All correct\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "def tensor_creation(Z):\n",
    "  \"\"\"\n",
    "  A function that creates various tensors.\n",
    "\n",
    "  Args:\n",
    "    Z: numpy.ndarray\n",
    "      An array of shape (3,4)\n",
    "\n",
    "  Returns:\n",
    "    A : Tensor\n",
    "      20 by 21 tensor consisting of ones\n",
    "    B : Tensor\n",
    "      A tensor with elements equal to the elements of numpy array  Z\n",
    "    C : Tensor\n",
    "      A tensor with the same number of elements as A but with values ∼U(0,1)\n",
    "    D : Tensor\n",
    "      A 1D tensor containing the even numbers between 4 and 40 inclusive.\n",
    "  \"\"\"\n",
    "\n",
    "  A = torch.ones(20, 21)\n",
    "  B = torch.tensor(Z)\n",
    "  C = torch.rand_like(A)\n",
    "  D = torch.arange(4, 41, step=2)\n",
    "\n",
    "  return A, B, C, D\n",
    "\n",
    "\n",
    "# numpy array to copy later\n",
    "Z = np.vander([1, 2, 3], 4)\n",
    "\n",
    "# Uncomment below to check your function!\n",
    "A, B, C, D = tensor_creation(Z)\n",
    "checkExercise1(A, B, C, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "csjVyp7xZt2L"
   },
   "source": [
    "```\n",
    "All correct!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "35QGDrJCZt2L"
   },
   "source": [
    "## Section 1.2: Operations in PyTorch\n",
    "\n",
    "**Tensor-Tensor operations**\n",
    "\n",
    "We can perform operations on tensors using methods under `torch.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {},
    "id": "8JOAze0_Zt2L",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "daca3f1b-2d55-4724-e086-061e5701c97a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a:  tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b:  tensor([[0.6464, 0.5228, 0.0491],\n",
      "        [0.9147, 0.7692, 0.9970],\n",
      "        [0.7526, 0.1700, 0.9173],\n",
      "        [0.5269, 0.7371, 0.0991],\n",
      "        [0.3562, 0.0091, 0.3053]])\n",
      "c:  tensor([[1.6464, 1.5228, 1.0491],\n",
      "        [1.9147, 1.7692, 1.9970],\n",
      "        [1.7526, 1.1700, 1.9173],\n",
      "        [1.5269, 1.7371, 1.0991],\n",
      "        [1.3562, 1.0091, 1.3053]])\n",
      "d:  tensor([[0.6464, 0.5228, 0.0491],\n",
      "        [0.9147, 0.7692, 0.9970],\n",
      "        [0.7526, 0.1700, 0.9173],\n",
      "        [0.5269, 0.7371, 0.0991],\n",
      "        [0.3562, 0.0091, 0.3053]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5, 3)\n",
    "b = torch.rand(5, 3)\n",
    "c = torch.empty(5, 3)\n",
    "d = torch.empty(5, 3)\n",
    "\n",
    "# this only works if c and d already exist\n",
    "torch.add(a, b, out=c)\n",
    "\n",
    "# Pointwise Multiplication of a and b\n",
    "torch.multiply(a, b, out=d)\n",
    "\n",
    "print('a: ', a)\n",
    "print('b: ', b)\n",
    "\n",
    "print('c: ', c)\n",
    "print('d: ', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "xjLXE2R7Zt2L"
   },
   "source": [
    "However, in PyTorch, most common Python operators are overridden.\n",
    "The common standard arithmetic operators ($+$, $-$, $*$, $/$, and $**$) have all been lifted to elementwise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {},
    "id": "0e7ZTGjbZt2L",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2e45eb0f-332e-466e-a347-7bf1b948dbd0"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([ 2,  4,  7, 12]),\n",
       " tensor([0, 0, 1, 4]),\n",
       " tensor([ 1,  4, 12, 32]),\n",
       " tensor([1.0000, 1.0000, 1.3333, 2.0000]),\n",
       " tensor([   1,    4,   64, 4096]))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 4, 8])\n",
    "y = torch.tensor([1, 2, 3, 4])\n",
    "x + y, x - y, x * y, x / y, x**y  # The `**` is the exponentiation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "fkBsWkRNZt2L"
   },
   "source": [
    "**Tensor Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "0zFWmfl3Zt2L"
   },
   "source": [
    "Tensors also have a number of common arithmetic operations built in. A full list of **all** methods can be found  in the appendix (there are a lot!)\n",
    "\n",
    "All of these operations should have similar syntax to their numpy equivalents (feel free to skip if you already know this!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {},
    "id": "TCcDYIkVZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "11b6ac45-acdd-49c1-aa3d-05f9570b80be"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.6079, 0.1074, 0.6594],\n",
      "        [0.7684, 0.5697, 0.1655],\n",
      "        [0.1123, 0.3457, 0.7195]])\n",
      "\n",
      "\n",
      "Sum of every element of x: 4.055744171142578\n",
      "Sum of the columns of x: tensor([1.4886, 1.0228, 1.5443])\n",
      "Sum of the rows of x: tensor([1.3747, 1.5035, 1.1776])\n",
      "\n",
      "\n",
      "Mean value of all elements of x 0.4506382346153259\n",
      "Mean values of the columns of x tensor([0.4962, 0.3409, 0.5148])\n",
      "Mean values of the rows of x tensor([0.4582, 0.5012, 0.3925])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3)\n",
    "print(x)\n",
    "print(\"\\n\")\n",
    "# sum() - note the axis is the axis you move across when summing\n",
    "print(f\"Sum of every element of x: {x.sum()}\")\n",
    "print(f\"Sum of the columns of x: {x.sum(axis=0)}\")\n",
    "print(f\"Sum of the rows of x: {x.sum(axis=1)}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Mean value of all elements of x {x.mean()}\")\n",
    "print(f\"Mean values of the columns of x {x.mean(axis=0)}\")\n",
    "print(f\"Mean values of the rows of x {x.mean(axis=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "O5NLwRfCZt2M"
   },
   "source": [
    "**Matrix Operations**\n",
    "\n",
    "The `@` symbol is overridden to represent matrix multiplication. You can also use `torch.matmul()` to multiply tensors. For dot multiplication, you can use `torch.dot()`, or manipulate the axes of your tensors and do matrix multiplication (we will cover that in the next section).\n",
    "\n",
    "Transposes of 2D tensors are obtained using `torch.t()` or `Tensor.T`. Note the lack of brackets for `Tensor.T` - it is an attribute, not a method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "DQkr8Fd6Zt2M"
   },
   "source": [
    "### Coding Exercise 1.2 : Simple tensor operations\n",
    "\n",
    "Below are two expressions involving operations on matrices.\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{A} =\n",
    "\\begin{bmatrix}2 &4 \\\\5 & 7\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} 1 &1 \\\\2 & 3\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}10 & 10  \\\\ 12 & 1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "b =\n",
    "\\begin{bmatrix} 3 \\\\ 5 \\\\ 7\n",
    "\\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix} 2 \\\\ 4 \\\\ 8\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The code block below that computes these expressions using PyTorch is incomplete - fill in the missing lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "40ssVqH_Zt2M"
   },
   "source": [
    "```python\n",
    "def simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate simple operations\n",
    "  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n",
    "\n",
    "  Args:\n",
    "    a1: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a2: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a3: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "\n",
    "  Returns:\n",
    "    answer: Torch tensor\n",
    "      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students:  complete the first computation using the argument matricies\n",
    "  raise NotImplementedError(\"Student exercise: fill in the missing code to complete the operation\")\n",
    "  ################################################\n",
    "  #\n",
    "  answer = ...\n",
    "  return answer\n",
    "\n",
    "\n",
    "# Computing expression 1:\n",
    "\n",
    "# init our tensors\n",
    "a1 = torch.tensor([[2, 4], [5, 7]])\n",
    "a2 = torch.tensor([[1, 1], [2, 3]])\n",
    "a3 = torch.tensor([[10, 10], [12, 1]])\n",
    "## uncomment to test your function\n",
    "# A = simple_operations(a1, a2, a3)\n",
    "# print(A)\n",
    "\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Activity 6\n",
    "Please, develop a solution to our previous exercise, and submit the top left number of the output A matrix following this template [your_name: value] on [AhaSlides](https://ahaslides.com/JAD9A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {},
    "id": "pxr6qac9Zt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "876fbe3d-b5f3-4c65-bf31-17b3d23c9b71"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[20, 24],\n",
      "        [31, 27]])\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "def simple_operations(a1: torch.Tensor, a2: torch.Tensor, a3: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate simple operations\n",
    "  i.e., Multiplication of tensor a1 with tensor a2 and then add it with tensor a3\n",
    "\n",
    "  Args:\n",
    "    a1: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a2: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "    a3: Torch tensor\n",
    "      Tensor of size ([2,2])\n",
    "\n",
    "  Returns:\n",
    "    answer: Torch tensor\n",
    "      Tensor of size ([2,2]) resulting from a1 multiplied with a2, added with a3\n",
    "  \"\"\"\n",
    "  answer = a1 @ a2 + a3\n",
    "  return answer\n",
    "\n",
    "\n",
    "# Computing expression 1:\n",
    "\n",
    "# init our tensors\n",
    "a1 = torch.tensor([[2, 4], [5, 7]])\n",
    "a2 = torch.tensor([[1, 1], [2, 3]])\n",
    "a3 = torch.tensor([[10, 10], [12, 1]])\n",
    "## uncomment to test your function\n",
    "A = simple_operations(a1, a2, a3)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "0STMFgFHZt2M"
   },
   "source": [
    "```\n",
    "tensor([[20, 24],\n",
    "        [31, 27]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "_91iZXGaZt2M"
   },
   "source": [
    "```python\n",
    "def dot_product(b1: torch.Tensor, b2: torch.Tensor):\n",
    "  ###############################################\n",
    "  ## TODO for students:  complete the first computation using the argument matricies\n",
    "  raise NotImplementedError(\"Student exercise: fill in the missing code to complete the operation\")\n",
    "  ###############################################\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate dot product operation\n",
    "  Dot product is an algebraic operation that takes two equal-length sequences\n",
    "  (usually coordinate vectors), and returns a single number.\n",
    "  Geometrically, it is the product of the Euclidean magnitudes of the\n",
    "  two vectors and the cosine of the angle between them.\n",
    "\n",
    "  Args:\n",
    "    b1: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "    b2: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "\n",
    "  Returns:\n",
    "    product: Tensor\n",
    "      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n",
    "  \"\"\"\n",
    "  # Use torch.dot() to compute the dot product of two tensors\n",
    "  product = ...\n",
    "  return product\n",
    "\n",
    "\n",
    "# Computing expression 2:\n",
    "b1 = torch.tensor([3, 5, 7])\n",
    "b2 = torch.tensor([2, 4, 8])\n",
    "## Uncomment to test your function\n",
    "# b = dot_product(b1, b2)\n",
    "# print(b)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {},
    "id": "G8VRHVJDZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "351b961c-bc5c-40db-a873-666990697d44"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(82)\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "def dot_product(b1: torch.Tensor, b2: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Helper function to demonstrate dot product operation\n",
    "  Dot product is an algebraic operation that takes two equal-length sequences\n",
    "  (usually coordinate vectors), and returns a single number.\n",
    "  Geometrically, it is the product of the Euclidean magnitudes of the\n",
    "  two vectors and the cosine of the angle between them.\n",
    "\n",
    "  Args:\n",
    "    b1: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "    b2: Torch tensor\n",
    "      Tensor of size ([3])\n",
    "\n",
    "  Returns:\n",
    "    product: Tensor\n",
    "      Tensor of size ([1]) resulting from b1 scalar multiplied with b2\n",
    "  \"\"\"\n",
    "  # Use torch.dot() to compute the dot product of two tensors\n",
    "  product = torch.dot(b1, b2)\n",
    "  return product\n",
    "\n",
    "\n",
    "# Computing expression 2:\n",
    "b1 = torch.tensor([3, 5, 7])\n",
    "b2 = torch.tensor([2, 4, 8])\n",
    "## Uncomment to test your function\n",
    "b = dot_product(b1, b2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "HbvWSwdkZt2M"
   },
   "source": [
    "```\n",
    "tensor(82)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "4oFrE2gGZt2M"
   },
   "source": [
    "## Section 1.3 Manipulating Tensors in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "_oZh7DlBZt2M"
   },
   "source": [
    "**Indexing**\n",
    "\n",
    "Just as in numpy, elements in a tensor can be accessed by index. As in any numpy array, the first element has index 0 and ranges are specified to include the first to last_element-1. We can access elements according to their relative position to the end of the list by using negative indices. Indexing is also referred to as slicing.\n",
    "\n",
    "For example, `[-1]` selects the last element; `[1:3]` selects the second and the third elements, and `[:-2]` will select all elements excluding the last and second-to-last elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {},
    "id": "IwZqIPYfZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a51b4fc5-b303-436a-ab89-44537b083987"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor(9)\n",
      "tensor([1, 2])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10)\n",
    "print(x)\n",
    "print(x[-1])\n",
    "print(x[1:3])\n",
    "print(x[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "LwbgLcO5Zt2M"
   },
   "source": [
    "When we have multidimensional tensors, indexing rules work the same way as NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {},
    "id": "AtYnculFZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e5a3bbd-bc4d-4434-9655-0942a41dedb9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " shape of x[0]:torch.Size([2, 3, 4, 5])\n",
      " shape of x[0][0]:torch.Size([3, 4, 5])\n",
      " shape of x[0][0][0]:torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# make a 5D tensor\n",
    "x = torch.rand(1, 2, 3, 4, 5)\n",
    "\n",
    "print(f\" shape of x[0]:{x[0].shape}\")\n",
    "print(f\" shape of x[0][0]:{x[0][0].shape}\")\n",
    "print(f\" shape of x[0][0][0]:{x[0][0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Cg43qpV2Zt2M"
   },
   "source": [
    "**Flatten and reshape**\n",
    "\n",
    "There are various methods for reshaping tensors. It is common to have to express 2D data in 1D format. Similarly, it is also common to have to reshape a 1D tensor into a 2D tensor. We can achieve this with the `.flatten()` and `.reshape()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {},
    "id": "XdOWAkIDZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "95f1b7c3-4579-478c-a2b0-5a4322ebf95e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original z: \n",
      " tensor([[ 0,  1],\n",
      "        [ 2,  3],\n",
      "        [ 4,  5],\n",
      "        [ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]])\n",
      "Flattened z: \n",
      " tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Reshaped (3x4) z: \n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.arange(12).reshape(6, 2)\n",
    "print(f\"Original z: \\n {z}\")\n",
    "\n",
    "# 2D -> 1D\n",
    "z = z.flatten()\n",
    "print(f\"Flattened z: \\n {z}\")\n",
    "\n",
    "# and back to 2D\n",
    "z = z.reshape(3, 4)\n",
    "print(f\"Reshaped (3x4) z: \\n {z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Kn5RUfmGZt2M"
   },
   "source": [
    "You will also see the `.view()` methods used a lot to reshape tensors. There is a subtle difference between `.view()` and `.reshape()`, though for now we will just use `.reshape()`. The documentation can be found in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "gLMlFDq4Zt2M"
   },
   "source": [
    "**Squeezing tensors**\n",
    "\n",
    "When processing batches of data, you will quite often be left with singleton dimensions. E.g., `[1,10]` or `[256, 1, 3]`. This dimension can quite easily mess up your matrix operations if you don't plan on it being there...\n",
    "\n",
    "In order to compress tensors along their singleton dimensions we can use the `.squeeze()` method. We can use the `.unsqueeze()` method to do the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {},
    "id": "3XwRNIuYZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cfca4dce-bcb2-474f-c559-c968d9c28481"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 10])\n",
      "x[0]: tensor([-1.6057,  0.2758, -0.0624, -1.5593, -0.8883, -0.7485,  0.1456, -0.6005,\n",
      "         0.8546,  1.1705])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10)\n",
    "# printing the zeroth element of the tensor will not give us the first number!\n",
    "\n",
    "print(x.shape)\n",
    "print(f\"x[0]: {x[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Wp1gXAFAZt2M"
   },
   "source": [
    "Because of that pesky singleton dimension, `x[0]` gave us the first row instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {},
    "id": "ykRhAgEYZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6ca595ba-d2e9-4e57-eb41-1ddee11870a6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10])\n",
      "x[0]: -1.6056749820709229\n"
     ]
    }
   ],
   "source": [
    "# Let's get rid of that singleton dimension and see what happens now\n",
    "x = x.squeeze(0)\n",
    "print(x.shape)\n",
    "print(f\"x[0]: {x[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {},
    "id": "Gjxoh0AwZt2M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97adca13-b1c0-4857-974c-1141511e5447"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape of y: torch.Size([5, 5])\n",
      "Shape of y: torch.Size([5, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Adding singleton dimensions works a similar way, and is often used when tensors\n",
    "# being added need same number of dimensions\n",
    "\n",
    "y = torch.randn(5, 5)\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# lets insert a singleton dimension\n",
    "y = y.unsqueeze(1)\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "07C6-KwSZt2N"
   },
   "source": [
    "**Permutation**\n",
    "\n",
    "Sometimes our dimensions will be in the wrong order! For example, we may be dealing with RGB images with dim $[3\\times48\\times64]$, but our pipeline expects the colour dimension to be the last dimension, i.e., $[48\\times64\\times3]$. To get around this we can use the `.permute()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {},
    "id": "nZMOFJRjZt2N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "624f4342-0bba-4668-983a-e71808f3c4aa"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([48, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# `x` has dimensions [color,image_height,image_width]\n",
    "x = torch.rand(3, 48, 64)\n",
    "\n",
    "# We want to permute our tensor to be [ image_height , image_width , color ]\n",
    "x = x.permute(1, 2, 0)\n",
    "# permute(1,2,0) means:\n",
    "# The 0th dim of my new tensor = the 1st dim of my old tensor\n",
    "# The 1st dim of my new tensor = the 2nd\n",
    "# The 2nd dim of my new tensor = the 0th\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "lR2NqqnIZt2N"
   },
   "source": [
    "You may also see `.transpose()` used. This works in a similar way as permute, but can only swap two dimensions at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "PL6K6oOwZt2N"
   },
   "source": [
    "**Concatenation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "CJvs8GOEZt2N"
   },
   "source": [
    "In this example, we concatenate two matrices along rows (axis 0, the first element of the shape) vs. columns (axis 1, the second element of the shape). We can see that the first output tensor’s axis-0 length (`6`) is the sum of the two input tensors’ axis-0 lengths (`3+3`); while the second output tensor’s axis-1 length (`8`) is the sum of the two input tensors’ axis-1 lengths (`4+4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {},
    "id": "4-XEYAi3Zt2N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fff9125d-58dd-40a8-8af7-c8e7960250bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Concatenated by rows: shape[6, 4] \n",
      " tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [ 2.,  1.,  4.,  3.],\n",
      "        [ 1.,  2.,  3.,  4.],\n",
      "        [ 4.,  3.,  2.,  1.]])\n",
      "\n",
      " Concatenated by colums: shape[3, 8]  \n",
      " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
      "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors of the same shape\n",
    "x = torch.arange(12, dtype=torch.float32).reshape((3, 4))\n",
    "y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "\n",
    "# Concatenate along rows\n",
    "cat_rows = torch.cat((x, y), dim=0)\n",
    "\n",
    "# Concatenate along columns\n",
    "cat_cols = torch.cat((x, y), dim=1)\n",
    "\n",
    "# Printing outputs\n",
    "print('Concatenated by rows: shape{} \\n {}'.format(list(cat_rows.shape), cat_rows))\n",
    "print('\\n Concatenated by colums: shape{}  \\n {}'.format(list(cat_cols.shape), cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "a0cQ_mkBZt2N"
   },
   "source": [
    "**Conversion to Other Python Objects**\n",
    "\n",
    "Converting a tensor to a numpy.ndarray, or vice versa, is easy, and the converted result does not share memory. This minor inconvenience is quite important: when you perform operations on the CPU or GPUs, you do not want to halt computation, waiting to see whether the NumPy package of Python might want to be doing something else with the same chunk of memory.\n",
    "\n",
    "When converting to a NumPy array, the information being tracked by the tensor will be lost, i.e., the computational graph. This will be covered in detail when you are introduced to autograd tomorrow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {},
    "id": "rccJ-AOXZt2N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dff33aca-fc8f-484d-f6cd-8e06fff80cc5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x: tensor([-2.2472,  0.2189, -0.6323,  0.9613,  0.5662])  |  x type:  torch.FloatTensor\n",
      "y: [-2.2472162   0.21886738 -0.6322729   0.96129405  0.5662092 ]  |  y type:  <class 'numpy.ndarray'>\n",
      "z: tensor([-2.2472,  0.2189, -0.6323,  0.9613,  0.5662])  |  z type:  torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "print(f\"x: {x}  |  x type:  {x.type()}\")\n",
    "\n",
    "y = x.numpy()\n",
    "print(f\"y: {y}  |  y type:  {type(y)}\")\n",
    "\n",
    "z = torch.tensor(y)\n",
    "print(f\"z: {z}  |  z type:  {z.type()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "cwV3xrA9Zt2N"
   },
   "source": [
    "To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {},
    "id": "ed8i9bolZt2N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d33cdb29-906d-4a6c-d7fc-5c86e0fa6714"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "ECb-VJP1Zt2N"
   },
   "source": [
    "### Coding Exercise 1.3: Manipulating Tensors\n",
    "Using a combination of the methods discussed above, complete the functions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "PJbc721vZt2N"
   },
   "source": [
    "**Function A**\n",
    "\n",
    "This function takes in two 2D tensors $A$ and $B$ and returns the column sum of A multiplied by the sum of all the elmements of $B$, i.e., a scalar, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  A = \\begin{bmatrix}\n",
    "  1 & 1 \\\\\n",
    "  1 & 1\n",
    "  \\end{bmatrix}\n",
    "  \\text{and }\n",
    "  B = \\begin{bmatrix}\n",
    "  1 & 2 & 3 \\\\\n",
    "  1 & 2 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out =  \\begin{bmatrix}\n",
    "  2 & 2\n",
    "  \\end{bmatrix} \\cdot 12 = \\begin{bmatrix}\n",
    "  24 & 24\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "**Function B**\n",
    "\n",
    "This function takes in a square matrix $C$ and returns a 2D tensor consisting of a flattened $C$ with the index of each element appended to this tensor in the row dimension, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  C = \\begin{bmatrix}\n",
    "  2 & 3 \\\\\n",
    "  -1 & 10\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out = \\begin{bmatrix}\n",
    "  0 & 2 \\\\\n",
    "  1 & 3 \\\\\n",
    "  2 & -1 \\\\\n",
    "  3 & 10\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "**Hint:** Pay close attention to singleton dimensions.\n",
    "\n",
    "**Function C**\n",
    "\n",
    "This function takes in two 2D tensors $D$ and $E$. If the dimensions allow it, this function returns the elementwise sum of $D$-shaped $E$, and $D$; else this function returns a 1D tensor that is the concatenation of the two tensors, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  D = \\begin{bmatrix}\n",
    "  1 & -1 \\\\\n",
    "  -1 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{and }\n",
    "  E = \\begin{bmatrix}\n",
    "  2 & 3 & 0 & 2 \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out = \\begin{bmatrix}\n",
    "  3 & 2 \\\\\n",
    "  -1 & 5\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{If }\n",
    "  D = \\begin{bmatrix}\n",
    "  1 & -1 \\\\\n",
    "  -1 & 3\n",
    "  \\end{bmatrix}\n",
    "  \\text{and }\n",
    "  E = \\begin{bmatrix}\n",
    "  2 & 3 & 0  \\\\\n",
    "  \\end{bmatrix}\n",
    "  \\text{ then }\n",
    "  Out = \\begin{bmatrix}\n",
    "  1 & -1 & -1 & 3  & 2 & 3 & 0\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "**Hint:** `torch.numel()` is an easy way of finding the number of elements in a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "bZUdD83GZt2N"
   },
   "source": [
    "```python\n",
    "def functionA(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`\n",
    "  and returns the column sum of\n",
    "  `my_tensor1` multiplied by the sum of all the elmements of `my_tensor2`,\n",
    "  i.e., a scalar.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Retuns:\n",
    "    output: torch.Tensor\n",
    "      The multiplication of the column sum of `my_tensor1` by the sum of\n",
    "      `my_tensor2`.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students: complete functionA\n",
    "  raise NotImplementedError(\"Student exercise: complete function A\")\n",
    "  ################################################\n",
    "  # TODO multiplication the sum of the tensors\n",
    "  output = ...\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionB(my_tensor):\n",
    "  \"\"\"\n",
    "  This function takes in a square matrix `my_tensor` and returns a 2D tensor\n",
    "  consisting of a flattened `my_tensor` with the index of each element\n",
    "  appended to this tensor in the row dimension.\n",
    "\n",
    "  Args:\n",
    "    my_tensor: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students: complete functionB\n",
    "  raise NotImplementedError(\"Student exercise: complete function B\")\n",
    "  ################################################\n",
    "  # TODO flatten the tensor `my_tensor`\n",
    "  my_tensor = ...\n",
    "  # TODO create the idx tensor to be concatenated to `my_tensor`\n",
    "  idx_tensor = ...\n",
    "  # TODO concatenate the two tensors\n",
    "  output = ...\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionC(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`.\n",
    "  If the dimensions allow it, it returns the\n",
    "  elementwise sum of `my_tensor1`-shaped `my_tensor2`, and `my_tensor2`;\n",
    "  else this function returns a 1D tensor that is the concatenation of the\n",
    "  two tensors.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  ################################################\n",
    "  ## TODO for students: complete functionB\n",
    "  raise NotImplementedError(\"Student exercise: complete function C\")\n",
    "  ################################################\n",
    "  # TODO check we can reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "  if ...:\n",
    "    # TODO reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "    my_tensor2 = ...\n",
    "    # TODO sum the two tensors\n",
    "    output = ...\n",
    "  else:\n",
    "    # TODO flatten both tensors\n",
    "    my_tensor1 = ...\n",
    "    my_tensor2 = ...\n",
    "    # TODO concatenate the two tensors in the correct dimension\n",
    "    output = ...\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "## Implement the functions above and then uncomment the following lines to test your code\n",
    "# print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))\n",
    "# print(functionB(torch.tensor([[2, 3], [-1, 10]])))\n",
    "# print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))\n",
    "# print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {},
    "id": "cT9Pal8bZt2N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a74bd88e-2bf1-450e-c97d-15a4f3d93bff"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([24, 24])\n",
      "tensor([[ 0,  2],\n",
      "        [ 1,  3],\n",
      "        [ 2, -1],\n",
      "        [ 3, 10]])\n",
      "tensor([[ 3,  2],\n",
      "        [-1,  5]])\n",
      "tensor([ 1, -1, -1,  3,  2,  3,  0])\n"
     ]
    }
   ],
   "source": [
    "# to_remove solution\n",
    "def functionA(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`\n",
    "  and returns the column sum of\n",
    "  `my_tensor1` multiplied by the sum of all the elmements of `my_tensor2`,\n",
    "  i.e., a scalar.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      The multiplication of the column sum of `my_tensor1` by the sum of\n",
    "      `my_tensor2`.\n",
    "  \"\"\"\n",
    "  # TODO multiplication the sum of the tensors\n",
    "  output = my_tensor1.sum(axis=0) * my_tensor2.sum()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionB(my_tensor):\n",
    "  \"\"\"\n",
    "  This function takes in a square matrix `my_tensor` and returns a 2D tensor\n",
    "  consisting of a flattened `my_tensor` with the index of each element\n",
    "  appended to this tensor in the row dimension.\n",
    "\n",
    "  Args:\n",
    "    my_tensor: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  # TODO flatten the tensor `my_tensor`\n",
    "  my_tensor = my_tensor.flatten()\n",
    "  # TODO create the idx tensor to be concatenated to `my_tensor`\n",
    "  idx_tensor = torch.arange(0, len(my_tensor))\n",
    "  # TODO concatenate the two tensors\n",
    "  output = torch.cat([idx_tensor.unsqueeze(1), my_tensor.unsqueeze(1)], axis=1)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "def functionC(my_tensor1, my_tensor2):\n",
    "  \"\"\"\n",
    "  This function takes in two 2D tensors `my_tensor1` and `my_tensor2`.\n",
    "  If the dimensions allow it, it returns the\n",
    "  elementwise sum of `my_tensor1`-shaped `my_tensor2`, and `my_tensor2`;\n",
    "  else this function returns a 1D tensor that is the concatenation of the\n",
    "  two tensors.\n",
    "\n",
    "  Args:\n",
    "    my_tensor1: torch.Tensor\n",
    "    my_tensor2: torch.Tensor\n",
    "\n",
    "  Returns:\n",
    "    output: torch.Tensor\n",
    "      Concatenated tensor.\n",
    "  \"\"\"\n",
    "  # TODO check we can reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "  if torch.numel(my_tensor1) == torch.numel(my_tensor2):\n",
    "    # TODO reshape `my_tensor2` into the shape of `my_tensor1`\n",
    "    my_tensor2 = my_tensor2.reshape(my_tensor1.shape)\n",
    "    # TODO sum the two tensors\n",
    "    output = my_tensor1 + my_tensor2\n",
    "  else:\n",
    "    # TODO flatten both tensors\n",
    "    my_tensor1 = my_tensor1.reshape(1, -1)\n",
    "    my_tensor2 = my_tensor2.reshape(1, -1)\n",
    "    # TODO concatenate the two tensors in the correct dimension\n",
    "    output = torch.cat([my_tensor1, my_tensor2], axis=1).squeeze()\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "## Implement the functions above and then uncomment the following lines to test your code\n",
    "print(functionA(torch.tensor([[1, 1], [1, 1]]), torch.tensor([[1, 2, 3], [1, 2, 3]])))\n",
    "print(functionB(torch.tensor([[2, 3], [-1, 10]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0, 2]])))\n",
    "print(functionC(torch.tensor([[1, -1], [-1, 3]]), torch.tensor([[2, 3, 0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "PpmSwijfZt2N"
   },
   "source": [
    "```\n",
    "tensor([24, 24])\n",
    "tensor([[ 0,  2],\n",
    "        [ 1,  3],\n",
    "        [ 2, -1],\n",
    "        [ 3, 10]])\n",
    "tensor([[ 3,  2],\n",
    "        [-1,  5]])\n",
    "tensor([ 1, -1, -1,  3,  2,  3,  0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Section 2: Autograd\n",
    "Pytorch is well-known for its automatic differentiation feature. We can call the `backward()` method to ask `PyTorch` to calculate the gradients, which are then stored in the `grad` attribute."
   ],
   "metadata": {
    "id": "ldTBJj5zqPk2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import pprint, module we use for making our print statements prettier\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ],
   "metadata": {
    "id": "SpQb2-cUF8O8"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Create an example tensor\n",
    "# requires_grad parameter tells PyTorch to store gradients\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# Print the gradient if it is calculated\n",
    "# Currently None since x is a scalar\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "id": "N1eaPn7qqVPS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c2c44666-a7f1-4be0-fbfa-a31314a04168"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculating the gradient of y with respect to x\n",
    "y = x * x * 3 # 3x^2\n",
    "y.backward()\n",
    "pp.pprint(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
   ],
   "metadata": {
    "id": "2QEDFYlaqXKC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e6890d93-d065-4ce4-dd3a-39b56fc01679"
   },
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's run backprop from a different tensor again to see what happens."
   ],
   "metadata": {
    "id": "YDMon7HQqZsc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "id": "z8IOj220qbf4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3bf9178e-e1e7-4519-93a1-716be28762a6"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x.grad = None\n",
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "# y = x * x * 3\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "id": "0OgYB3ULqdK0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "529777dd-f9b8-45c9-fdad-8da826f8022c"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "# y = x * x * 3\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "id": "lvJ1fPgAqe6P",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8921cef6-33fd-4aa1-a333-43cf86a6da52"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "z = x * x * 3 # 3x^2\n",
    "z.backward()\n",
    "# y = x * x * 3\n",
    "pp.pprint(x.grad)"
   ],
   "metadata": {
    "id": "hiCFpa2Gqgcg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d5c76ee8-2f1b-472b-a2f4-7766e0e50fe0"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([36.])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
   ],
   "metadata": {
    "id": "hgy4QxQMqixf"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "hBvCqNn8Zt2S"
   },
   "source": [
    "---\n",
    "# Appendix\n",
    "\n",
    "## Official PyTorch resources:\n",
    "\n",
    "### Tutorials\n",
    "- [https://pytorch.org/tutorials/](https://pytorch.org/tutorials/)\n",
    "\n",
    "### Documentation\n",
    "- [https://pytorch.org/docs/stable/tensors.html](https://pytorch.org/docs/stable/tensors.html) (tensor methods)\n",
    "\n",
    "- [https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view) (The view method in particular)\n",
    "\n",
    "- [https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html) (pre-loaded image datasets)\n",
    "\n",
    "## Google Colab Resources:\n",
    "- [https://research.google.com/colaboratory/faq.html](https://research.google.com/colaboratory/faq.html) (FAQ including guidance on GPU usage)\n",
    "\n",
    "## Books for reference:\n",
    "- [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/) (Deep Learning by Ian Goodfellow, Yoshua Bengio and Aaron Courville)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Acknowledgement\n",
    "This Lab material is based on [Neuromatch](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials) and [Pytorch Stanford Tutorial](https://colab.research.google.com/drive/1Pz8b_h-W9zIBk1p2e6v-YFYThG1NkYeS?usp=sharing)"
   ],
   "metadata": {
    "id": "KBTOTH0QbN-N"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
